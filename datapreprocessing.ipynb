{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# import ace_tools as tools\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"data/2024-09-20_data (2)/parametric_modulation_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_819412/4102939616.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# calculate the gradient of the data\n",
    "columns = [\n",
    "    \"Date\", \"Time\", \"B_freq\", \"B_ampl\", \n",
    "    \"x_offset\", \"y_offset\", \"z_offset\",\n",
    "    \"g1_x\", \"g1_y\", \"g1_z\", \"g2_x\", \"g2_y\", \"g2_z\",\n",
    "    \n",
    "]\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Constants for gradient calculations\n",
    "CONST_B1_XY = 76.8500949\n",
    "CONST_B1_Z = 1927.272727\n",
    "CONST_B2_XY = 76.8500949\n",
    "CONST_B2_Z = 1927.272727\n",
    "\n",
    "# Iterate over the data in chunks of 7 rows\n",
    "for i in range(0, len(data), 7):\n",
    "    chunk = data.iloc[i:i+7]\n",
    "    if len(chunk) < 7:\n",
    "        break  # Skip incomplete chunks\n",
    "\n",
    "    # Extract relevant data\n",
    "    date = chunk.iloc[0]['Date']\n",
    "    time = chunk.iloc[0]['Time']\n",
    "    b_freq = chunk.iloc[0]['B_freq']\n",
    "    b_ampl = chunk.iloc[0]['B_ampl']\n",
    "    x_offset = chunk.iloc[0]['x_offset']\n",
    "    y_offset = chunk.iloc[0]['y_offset']\n",
    "    z_offset = chunk.iloc[0]['z_offset']\n",
    "\n",
    "    # Determine rows to use for gradient calculations\n",
    "    rows_for_gx = [5, 6]  # Use rows 1, 6, 7 for g_x calculation\n",
    "    rows_for_gy = [1, 2]  # Use rows 1, 2, 3 for g_y calculation\n",
    "    rows_for_gz = [3, 4]  # Use rows 1, 4, 5 for g_z calculation\n",
    "\n",
    "    # Helper function to calculate gradient while avoiding division by near-zero values\n",
    "    def calculate_gradient(delta_offset, delta_r, constant):\n",
    "        valid_indices = np.where(np.abs(delta_r) > 1e-9)[0]  # Filter out near-zero differences\n",
    "        if len(valid_indices) == 0:\n",
    "            return 0  # Return 0 if no valid differences are found\n",
    "        return (delta_r[valid_indices]/(delta_offset[valid_indices] * constant) ).mean() # change it \n",
    "\n",
    "    # Calculate gradients for B1 (using R1)\n",
    "    delta_r1_gx = chunk['R1'].iloc[rows_for_gx].diff().iloc[1:].values\n",
    "    delta_r1_gy = chunk['R1'].iloc[rows_for_gy].diff().iloc[1:].values\n",
    "    delta_r1_gz = chunk['R1'].iloc[rows_for_gz].diff().iloc[1:].values\n",
    "\n",
    "    delta_x_offset = chunk['x_offset'].iloc[rows_for_gx].diff().iloc[1:].values\n",
    "    delta_y_offset = chunk['y_offset'].iloc[rows_for_gy].diff().iloc[1:].values\n",
    "    delta_z_offset = chunk['z_offset'].iloc[rows_for_gz].diff().iloc[1:].values\n",
    "\n",
    "    g1_x = calculate_gradient(delta_x_offset, delta_r1_gx, CONST_B1_XY)\n",
    "    g1_y = calculate_gradient(delta_y_offset, delta_r1_gy, CONST_B1_XY)\n",
    "    g1_z = calculate_gradient(delta_z_offset, delta_r1_gz, CONST_B1_Z)\n",
    "\n",
    "    # Calculate gradients for B2 (using R2)\n",
    "    delta_r2_gx = chunk['R2'].iloc[rows_for_gx].diff().iloc[1:].values\n",
    "    delta_r2_gy = chunk['R2'].iloc[rows_for_gy].diff().iloc[1:].values\n",
    "    delta_r2_gz = chunk['R2'].iloc[rows_for_gz].diff().iloc[1:].values\n",
    "\n",
    "    g2_x = calculate_gradient(delta_x_offset, delta_r2_gx, CONST_B2_XY)\n",
    "    g2_y = calculate_gradient(delta_y_offset, delta_r2_gy, CONST_B2_XY)\n",
    "    g2_z = calculate_gradient(delta_z_offset, delta_r2_gz, CONST_B2_Z)\n",
    "\n",
    "    # Append results to the DataFrame\n",
    "    new_row = pd.DataFrame({\n",
    "        \"Date\": [date],\n",
    "        \"Time\": [time],\n",
    "        \"B_freq\": [b_freq],\n",
    "        \"B_ampl\": [b_ampl],\n",
    "        \"x_offset\": [x_offset],\n",
    "        \"y_offset\": [y_offset],\n",
    "        \"z_offset\": [z_offset],\n",
    "        \"g1_x\": [g1_x],\n",
    "        \"g1_y\": [g1_y],\n",
    "        \"g1_z\": [g1_z],\n",
    "        \"g2_x\": [g2_x],\n",
    "        \"g2_y\": [g2_y],\n",
    "        \"g2_z\": [g2_z]\n",
    "        \n",
    "    })\n",
    "    results = pd.concat([results, new_row], ignore_index=True)\n",
    "\n",
    "output_csv_path = '/home/guoguo/projects/QML/data/gradient_results.csv'\n",
    "results.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision control for dataset\n",
    "# load the gradient data\n",
    "data = pd.read_csv(output_csv_path)\n",
    "# Preprocess the input and output data with different precision\n",
    "data['B_freq'] = data['B_freq'].astype(int)  # Keep as integer\n",
    "data['B_ampl'] = data['B_ampl'].round(4)  # Round to 5 decimal places\n",
    "for col in ['x_offset', 'y_offset', 'z_offset']:\n",
    "    data[col] = data[col].round(4)  # Round to 3 decimal places\n",
    "for col in ['g1_x', 'g1_y', 'g1_z', 'g2_x', 'g2_y', 'g2_z']:\n",
    "    data[col] = data[col].round(9)  # Round to 5 decimal places\n",
    "# Take the absolute value of the output data\n",
    "# data[['g1_x', 'g1_y', 'g1_z', 'g2_x', 'g2_y', 'g2_z']] = data[['g1_x', 'g1_y', 'g1_z', 'g2_x', 'g2_y', 'g2_z']].abs()\n",
    "\n",
    "# save the percision processed data Xï¼Œ Y into a same csv file for later use\n",
    "data.to_csv('/home/guoguo/projects/QML/data/gradient_under_precision.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "scaler = StandardScaler()\n",
    "data_col = data.columns[2:]\n",
    "data[data_col] = scaler.fit_transform(data[data_col])\n",
    "# save the scaled data into a csv file\n",
    "data.to_csv('/home/guoguo/projects/QML/data/gradient_scaled.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
